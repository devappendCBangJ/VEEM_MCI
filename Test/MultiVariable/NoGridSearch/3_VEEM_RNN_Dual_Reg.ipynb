{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03efac7",
   "metadata": {},
   "source": [
    "## 0. 학습 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb85cfe",
   "metadata": {},
   "source": [
    "### 1) 메모리 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1858881c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\BangEnv\\lib\\site-packages\\torch\\__init__.py:617\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\BangEnv\\lib\\site-packages\\torch\\hub.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlparse  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm  \u001b[38;5;66;03m# automatically select proper tqdm submodule if available\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:982\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1423\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1395\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1522\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:142\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eeaffc",
   "metadata": {},
   "source": [
    "### 2) 수정된 코드 자동 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import foolbox as fb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bdaec7",
   "metadata": {},
   "source": [
    "## 1. Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 호출\n",
    "import os\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import argparse\n",
    "import easydict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from src.engines import train, evaluate, epoch_time\n",
    "from src.utils import load_checkpoint, save_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b3a52",
   "metadata": {},
   "source": [
    "## 2. Variable Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jupyter 외 환경\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--title\", type=str, default=\"baseline\")\n",
    "# parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "# parser.add_argument(\"--root\", type=str, default=\"data\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "# parser.add_argument(\"--num_workers\", type=int, default=2)\n",
    "# parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "# parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "# parser.add_argument(\"--logs\", type=str, default='logs')\n",
    "# parser.add_argument(\"--checkpoints\", type=str, default='checkpoints')\n",
    "# parser.add_argument(\"--resume\", type=bool, default=False)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Jupyter 환경\n",
    "args = easydict.EasyDict({\n",
    "        \"title\" : \"VEEM_RNN_Reg\",\n",
    "        \"learn_type\" : \"regression\",\n",
    "        \"device\" : \"cuda\",\n",
    "        \"batch_size\" : 32, # !!!\n",
    "        \"num_workers\" : 2,\n",
    "        \"epochs\" : 50, # !!!### 2) 모델 + 옵티마이저 + 손실함수 + 스케쥴러 + 메트릭 함수 정의\n",
    "        \"lr\" : 0.01, # !!!\n",
    "        \"logs\" : \"logs\",\n",
    "        \"checkpoints\" : \"checkpoints\",\n",
    "        \"resume\" : False,\n",
    "        \"test_ratio\" : 0.25,\n",
    "        \"input_size\" : 11,\n",
    "        \"hidden_size\" : 2,\n",
    "        \"num_layers\" : 1,\n",
    "        \"output_size\" : 5,\n",
    "        \"regression_output_size\" : 5,\n",
    "        \"classification_output_size\" : 2,\n",
    "        \"regression_lr\" : 0.01,\n",
    "        \"classification_lr\" : 3e-4,\n",
    "        \"regression_epochs\" : 500,\n",
    "        \"classification_epochs\" : 500\n",
    "    })\n",
    "\n",
    "if(args.learn_type == \"regression\"):\n",
    "    args.output_size = args.regression_output_size\n",
    "    args.lr = args.regression_lr\n",
    "    args.epochs = args.regression_epochs\n",
    "else:\n",
    "    args.output_size = args.classification_output_size\n",
    "    args.lr = args.classification_lr\n",
    "    args.epochs = args.classification_epochs\n",
    "\n",
    "\"\"\"\n",
    "print(args.output_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccbfd5",
   "metadata": {},
   "source": [
    "## 3. Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db39bf",
   "metadata": {},
   "source": [
    "### 1) 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x): # x : [batch_size, sequence_length, input_size]\n",
    "        \"\"\"\n",
    "        print(\"x.shape : \", x.shape)\n",
    "        print(\"x : \", x)\n",
    "        \"\"\"\n",
    "        # hidden state + cell state 초기화 (Bi-directional LSTM : 아래의 hidden and cell states의 첫번째 차원은 2*self.num_layers)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(args.device) # h0 : [num_layers, batch_size, hidden_size]\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(args.device) # c0 : [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        print(\"h0.shape : \", h0.shape)\n",
    "        print(\"c0.shape : \", c0.shape)\n",
    "        \n",
    "        print(\"h0 : \", h0)\n",
    "        print(\"c0 : \", c0)\n",
    "        \"\"\"\n",
    "\n",
    "        # LSTM 순전파\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0)) # out : [batch_size, sequence_length, hidden_size]\n",
    "        \"\"\"\n",
    "        print(\"out.shape : \", out.shape)\n",
    "        print(\"out : \", out)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 마지막 time step(sequence length)의 hidden state 반환\n",
    "        \"\"\"\n",
    "        print(\"out[:, -1, :] : \", out[:, -1, :])\n",
    "        \"\"\"\n",
    "        out = self.fc(out[:, -1, :]) # out : [batch_size, hidden_size] -> out : [batch_size, output_size]\n",
    "        \"\"\"\n",
    "        print(\"out.shape : \", out.shape)\n",
    "        print(\"out : \", out)\n",
    "        \"\"\"\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551357fe",
   "metadata": {},
   "source": [
    "### 2) 모델 + 옵티마이저 + 손실함수 + 스케쥴러 + 메트릭 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = RNN(args.input_size, args.hidden_size, args.num_layers, args.output_size).to(args.device)\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# Build scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs  * 37)\n",
    "\n",
    "# Build loss function + Build metric function\n",
    "if(args.learn_type == \"classification\"):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    metric_fn = accuracy\n",
    "else:\n",
    "    loss_fn = nn.MSELoss()\n",
    "    metric_fn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac16dcc",
   "metadata": {},
   "source": [
    "### 3) loggger 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build logger\n",
    "train_logger = SummaryWriter(f'{args.logs}/train/{args.title}')\n",
    "test_logger = SummaryWriter(f'{args.logs}/test/{args.title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01be75",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ffc5f5",
   "metadata": {},
   "source": [
    "## - Person, SNSB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a19bf",
   "metadata": {},
   "source": [
    "### 1) 데이터셋 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8bfea",
   "metadata": {},
   "source": [
    "### 2) 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7e913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터셋 불러오기 + 출력\n",
    "Person_path = f'C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\VEEM_Project\\\\data\\\\Person_SNSB\\\\VEEM 대상자 정보.csv'\n",
    "SNSB_path = f'C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\VEEM_Project\\\\data\\\\Person_SNSB\\\\VEEM SNSB 데이터.csv'\n",
    "\n",
    "Person_dataset=pd.read_csv(Person_path)\n",
    "SNSB_dataset=pd.read_csv(SNSB_path)\n",
    "\n",
    "print(\"ㅡㅡㅡㅡㅡ[Person_dataset.dtypes]ㅡㅡㅡㅡㅡ\")\n",
    "print(Person_dataset.dtypes)\n",
    "print(\"ㅡㅡㅡㅡㅡ[SNSB_dataset.dtypes]ㅡㅡㅡㅡㅡ\")\n",
    "print(SNSB_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a820279",
   "metadata": {},
   "source": [
    "### 3) 데이터 자료형 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Person_dataset['나이'] = Person_dataset['나이'].astype(float)\n",
    "Person_dataset['교육연한'] = Person_dataset['교육연한'].astype(float)\n",
    "\n",
    "SNSB_dataset['DST_F+B'] = SNSB_dataset['DST_F+B'].astype(float)\n",
    "SNSB_dataset['S-K-BNT'] = SNSB_dataset['S-K-BNT'].astype(float)\n",
    "SNSB_dataset['SVLT_delayedrecall'] = SNSB_dataset['SVLT_delayedrecall'].astype(float)\n",
    "SNSB_dataset['K-TMT-E_B'] = SNSB_dataset['K-TMT-E_B'].astype(float)\n",
    "\n",
    "print(\"ㅡㅡㅡㅡㅡ[Person_dataset.dtypes]ㅡㅡㅡㅡㅡ\")\n",
    "print(Person_dataset.dtypes)\n",
    "print(\"ㅡㅡㅡㅡㅡ[SNSB_dataset.dtypes]ㅡㅡㅡㅡㅡ\")\n",
    "print(SNSB_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ecd35",
   "metadata": {},
   "source": [
    "### 4) 인덱스 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Person_dataset.set_index('번호', inplace=True)\n",
    "SNSB_dataset.set_index('번호', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f40e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ㅡㅡㅡㅡㅡ[Person_dataset]ㅡㅡㅡㅡㅡ\")\n",
    "Person_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39186fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ㅡㅡㅡㅡㅡ[SNSB_dataset]ㅡㅡㅡㅡㅡ\")\n",
    "SNSB_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d24dab",
   "metadata": {},
   "source": [
    "### 5) 필요한 피쳐 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d91e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터셋 나누기\n",
    "SNSB_all_dataset=SNSB_dataset.iloc[:, 2:7]\n",
    "SNSB_all_dataset = SNSB_all_dataset.dropna(axis = 0)\n",
    "Person_all_dataset=Person_dataset.iloc[:, 2:]\n",
    "Person_dementia_dataset=Person_dataset.iloc[:, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c83ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ㅡㅡㅡㅡㅡ[SNSB_all_dataset]ㅡㅡㅡㅡㅡ\")\n",
    "SNSB_all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e575c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ㅡㅡㅡㅡㅡ[Person_all_dataset]ㅡㅡㅡㅡㅡ\")\n",
    "Person_all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60f1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ㅡㅡㅡㅡㅡ[Person_dementia_dataset]ㅡㅡㅡㅡㅡ\")\n",
    "Person_dementia_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba0348",
   "metadata": {},
   "source": [
    "### 6) 데이터 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd4eb6",
   "metadata": {},
   "source": [
    "#### (1) Person_dementia_dataset : pandas -> list -> 문자열 임베딩 -> torch 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in Person_dementia_dataset.index:\n",
    "#     print(Person_dementia_dataset.loc[idx,'집단'])\n",
    "\n",
    "# pandas -> list\n",
    "Person_dementia_dataset_list = Person_dementia_dataset['집단'].values.tolist()\n",
    "print(\"Person_dementia_dataset_list : \", Person_dementia_dataset_list)\n",
    "\n",
    "# list -> 문자열 임베딩\n",
    "embedding_table = {'HC': 0, 'MCI': 1}\n",
    "Person_dementia_dataset_embedding_list = []\n",
    "for i, word in enumerate(Person_dementia_dataset_list):\n",
    "    Person_dementia_dataset_embedding_list.append(embedding_table[word])\n",
    "print(\"Person_dementia_dataset_embedding_list : \", Person_dementia_dataset_embedding_list)\n",
    "\n",
    "# list -> torch 변환\n",
    "Person_dementia_dataset_torch = torch.Tensor(Person_dementia_dataset_embedding_list)\n",
    "print(\"Person_dementia_dataset_torch : \", Person_dementia_dataset_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d0eda",
   "metadata": {},
   "source": [
    "#### (2) SNSB_all_dataset : pandas -> numpy -> torch 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aff0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNSB_all_dataset_np = SNSB_all_dataset.to_numpy()\n",
    "SNSB_all_dataset_torch = torch.from_numpy(SNSB_all_dataset_np).float()\n",
    "\"\"\"\n",
    "print(\"ㅡㅡㅡㅡㅡ[SNSB_all_dataset_torch.shape]ㅡㅡㅡㅡㅡ\")\n",
    "print(SNSB_all_dataset_torch.shape)\n",
    "\n",
    "print(\"ㅡㅡㅡㅡㅡ[SNSB_all_dataset_torch]ㅡㅡㅡㅡㅡ\")\n",
    "print(SNSB_all_dataset_torch)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdf28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person_all_dataset_np = Person_all_dataset.to_numpy()\n",
    "# Person_all_dataset_torch = torch.from_numpy(Person_all_dataset_np).float()\n",
    "\n",
    "# print(\"ㅡㅡㅡㅡㅡ[Person_all_dataset_torch.shape]ㅡㅡㅡㅡㅡ\")\n",
    "# print(Person_all_dataset_torch.shape)\n",
    "\n",
    "# print(\"ㅡㅡㅡㅡㅡ[Person_all_dataset_torch]ㅡㅡㅡㅡㅡ\")\n",
    "# print(Person_all_dataset_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199977fb",
   "metadata": {},
   "source": [
    "## - eyerpt, rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bff87",
   "metadata": {},
   "source": [
    "### 1) 데이터셋 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb7936",
   "metadata": {},
   "source": [
    "### 2) 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23070805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 정보\n",
    "train_eyerpt_all_dataset = defaultdict(list)\n",
    "test_eyerpt_all_dataset = defaultdict(list)\n",
    "train_rpt_all_dataset = defaultdict(list)\n",
    "test_rpt_all_dataset = defaultdict(list)\n",
    "\n",
    "# 전체 경로\n",
    "eyerpt_rpt_path = f'C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\VEEM_Project\\\\data\\\\rpt\\\\'\n",
    "\n",
    "# 전체 폴더 내 파일 리스트 추출\n",
    "eyerpt_rpt_files_name = os.listdir(eyerpt_rpt_path)\n",
    "eyerpt_rpt_files_name = sorted(eyerpt_rpt_files_name)\n",
    "\n",
    "print(\"eyerpt_rpt_files_name : \", eyerpt_rpt_files_name)\n",
    "\n",
    "# eyerpt, rpt 파일 리스트 추출\n",
    "eyerpt_files_name = [eyerpt_rpt_file_name for eyerpt_rpt_file_name in eyerpt_rpt_files_name if \"eye\" in eyerpt_rpt_file_name]\n",
    "rpt_files_name = [eyerpt_rpt_file_name for eyerpt_rpt_file_name in eyerpt_rpt_files_name if not \"eye\" in eyerpt_rpt_file_name]\n",
    "\n",
    "print(\"eyerpt_files_name : \", eyerpt_files_name)\n",
    "print(\"rpt_files_name : \", rpt_files_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202b30d",
   "metadata": {},
   "source": [
    "### 3) train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ae851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 개수 -> 비율 기반 split\n",
    "all_eyerpt_count = len(eyerpt_files_name)\n",
    "test_eyerpt_count = int(all_eyerpt_count * args.test_ratio)\n",
    "train_eyerpt_count = all_eyerpt_count - test_eyerpt_count\n",
    "\n",
    "train_eyerpt_files_name = eyerpt_files_name[:train_eyerpt_count]\n",
    "test_eyerpt_files_name = eyerpt_files_name[train_eyerpt_count:]\n",
    "\"\"\"\n",
    "print(\"train_eyerpt_files_name : \", train_eyerpt_files_name)\n",
    "print(\"len(train_eyerpt_files_name) : \", len(train_eyerpt_files_name))\n",
    "print(\"test_eyerpt_files_name : \", test_eyerpt_files_name)\n",
    "print(\"len(test_eyerpt_files_name) : \", len(test_eyerpt_files_name))\n",
    "\"\"\"\n",
    "\n",
    "all_rpt_count = len(rpt_files_name)\n",
    "test_rpt_count = int(all_rpt_count * args.test_ratio)\n",
    "train_rpt_count = all_rpt_count - test_rpt_count\n",
    "\n",
    "train_rpt_files_name = rpt_files_name[:train_rpt_count]\n",
    "test_rpt_files_name = rpt_files_name[train_rpt_count:]\n",
    "\"\"\"\n",
    "print(\"train_rpt_files_name : \", train_rpt_files_name)\n",
    "print(\"len(train_rpt_files_name) : \", len(train_rpt_files_name))\n",
    "print(\"test_rpt_files_name : \", test_rpt_files_name)\n",
    "print(\"len(test_rpt_files_name) : \", len(test_rpt_files_name))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ac1c5",
   "metadata": {},
   "source": [
    "### 4) 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02356660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eyerpt 파일 전처리\n",
    "def eyerpt_rpt_preprocessing(name):\n",
    "    # =====================================================\n",
    "    # (1) 변수 불러오기\n",
    "    # =====================================================\n",
    "    print(\"name : \", name)\n",
    "    files_name = eval(f\"{name}_files_name\")\n",
    "    all_dataset = eval(f\"{name}_all_dataset\")\n",
    "\n",
    "    for i, file_name in enumerate(files_name):\n",
    "        # =====================================================\n",
    "        # (2) 데이터 불러오기\n",
    "        # =====================================================\n",
    "        path = eyerpt_rpt_path + file_name\n",
    "        \"\"\"\n",
    "        print('path : ', path) # 확인용 코드\n",
    "        \"\"\"\n",
    "\n",
    "        dataset=pd.read_csv(path)\n",
    "        \"\"\"\n",
    "        print(\"ㅡㅡㅡㅡㅡ[dataset.dtypes]ㅡㅡㅡㅡㅡ\")\n",
    "        print(dataset.dtypes)\n",
    "        \"\"\"\n",
    "\n",
    "        # =====================================================\n",
    "        # (3) 데이터 자료형 변환\n",
    "        # =====================================================\n",
    "\n",
    "        # =====================================================\n",
    "        # (4) 인덱스 지정\n",
    "        # =====================================================\n",
    "        # dataset['time stamp'] = pd.to_datetime(dataset['time stamp'])\n",
    "        dataset.set_index('time stamp', inplace=True)\n",
    "        \"\"\"\n",
    "        print(\"ㅡㅡㅡㅡㅡ[dataset]ㅡㅡㅡㅡㅡ\")\n",
    "        print(dataset)\n",
    "        \"\"\"\n",
    "\n",
    "        # =====================================================\n",
    "        # (5) 데이터 프레임 변환 + 필요한 피쳐 추출\n",
    "        # =====================================================\n",
    "        df = dataset.loc[:, :]\n",
    "        if(name == \"train_eyerpt\" or name == \"test_eyerpt\"):\n",
    "            df = df.drop(['time', 'beforeOBJ', 'presentOBJ', 'Obeject_name'], axis = 1) # 1 = columns\n",
    "        else:\n",
    "            df = df.drop(['total_task_time_s', 'hand_x_rotation_deg', 'hand_y_rotation_deg', 'hand_z_rotation_deg'], axis = 1)\n",
    "        \"\"\"\n",
    "        print(\"ㅡㅡㅡㅡㅡ[df]ㅡㅡㅡㅡㅡ\")\n",
    "        print(df)\n",
    "        \"\"\"\n",
    "\n",
    "        # =====================================================\n",
    "        # (6) 결측치 행 제거\n",
    "        # =====================================================\n",
    "        df.dropna(axis=0, inplace = True)\n",
    "        \"\"\"\n",
    "        print(\"ㅡㅡㅡㅡㅡ[df]ㅡㅡㅡㅡㅡ\")\n",
    "        print(df)\n",
    "        print(df.Panel_num)\n",
    "        \"\"\"\n",
    "\n",
    "        # =====================================================\n",
    "        # (7) 실험 종료 이후 데이터 제거\n",
    "        # =====================================================\n",
    "        if(name == \"train_eyerpt\" or name == \"test_eyerpt\"):\n",
    "            df_drop8 = df[df.Panel_num < 8]\n",
    "        else:\n",
    "            df_drop8 = df[df.panel_num < 8]\n",
    "        \"\"\"\n",
    "        print(\"ㅡㅡㅡㅡㅡ[df_drop8]ㅡㅡㅡㅡㅡ\")\n",
    "        print(df_drop8)\n",
    "        \"\"\"\n",
    "\n",
    "        # =====================================================\n",
    "        # (8) 데이터 프레임 -> numpy 변환 -> torch 변환\n",
    "        # =====================================================\n",
    "        if(name == \"train_eyerpt\" or name == \"test_eyerpt\"):\n",
    "            df_drop8_np = df_drop8.to_numpy()\n",
    "            df_drop8_torch = torch.from_numpy(df_drop8_np)\n",
    "        else:\n",
    "            df_drop8_np = df_drop8.to_numpy()\n",
    "            df_drop8_torch = torch.from_numpy(df_drop8_np).float()\n",
    "        \"\"\"\n",
    "        print(\"ㅡㅡㅡㅡㅡ[df_drop8_torch.shape]ㅡㅡㅡㅡㅡ\")\n",
    "        print(df_drop8_torch.shape)\n",
    "\n",
    "        print(\"ㅡㅡㅡㅡㅡ[df_drop8_torch]ㅡㅡㅡㅡㅡ\")\n",
    "        print(df_drop8_torch)\n",
    "        \"\"\"\n",
    "\n",
    "        # =====================================================\n",
    "        # (9) 데이터셋 길이 추출\n",
    "        # =====================================================\n",
    "        df_sequence_length = len(df_drop8_torch)\n",
    "        \"\"\"\n",
    "        print(\"df_sequence_length : \", df_sequence_length)\n",
    "        \"\"\"\n",
    "        \n",
    "        # =====================================================\n",
    "        # (10) 데이터셋 라벨 행 추출(float형 사용)\n",
    "        # =====================================================\n",
    "        if(args.learn_type == \"regression\"):\n",
    "            # =====================================================\n",
    "            # 1] SNSB 데이터셋 라벨 행 추출(float형 사용)\n",
    "            # =====================================================\n",
    "            SNSB_label = SNSB_all_dataset_torch[i,:]\n",
    "            \"\"\"\n",
    "            print(\"SNSB_label : \", SNSB_label)\n",
    "            \"\"\"\n",
    "            SNSB_label = SNSB_label.reshape(-1, len(SNSB_label)) # loss 학습을 위해 output과 형식 통일!!!\n",
    "            \"\"\"\n",
    "            print(\"SNSB_label : \", SNSB_label)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # =====================================================\n",
    "            # 2] Person_dementia 데이터셋 라벨 행 추출(float형 사용)\n",
    "            # =====================================================\n",
    "            Person_dementia_label = Person_dementia_dataset_torch[i]\n",
    "            \"\"\"\n",
    "            print(\"Person_dementia_label : \", Person_dementia_label)\n",
    "            \"\"\"\n",
    "            Person_dementia_label = Person_dementia_label.reshape(1) # loss 학습을 위해 output과 형식 통일!!!\n",
    "            \"\"\"\n",
    "            print(\"Person_dementia_label : \", Person_dementia_label)\n",
    "            \"\"\"\n",
    "\n",
    "        # =====================================================\n",
    "        # (11) 전체 데이터셋 구성\n",
    "        # =====================================================\n",
    "        # 파일 정보 + 파일 sequence 길이 리스트화\n",
    "        if(args.learn_type == \"regression\"):\n",
    "            df_infor = [df_drop8_torch, df_sequence_length, SNSB_label]\n",
    "        else:\n",
    "            df_infor = [df_drop8_torch, df_sequence_length, Person_dementia_label]\n",
    "        \"\"\"\n",
    "        print(\"df_infor : \", df_infor)\n",
    "        \"\"\"\n",
    "\n",
    "        # 모든 정보 딕셔너리화\n",
    "        all_dataset[file_name[0:2]].append(df_infor)\n",
    "        # *all_dataset = dict(zip([file_name[0:2]], df_infor))\n",
    "        \"\"\"\n",
    "        print(\"all_dataset['0'] : \", all_dataset['02'])\n",
    "        print(\"all_dataset : \", all_dataset)\n",
    "        \"\"\"\n",
    "\n",
    "    # =====================================================\n",
    "    # (12) 전체 데이터셋 return\n",
    "    # =====================================================\n",
    "    exec(f\"{name}_all_dataset = all_dataset\")\n",
    "\n",
    "    print(\"len(all_dataset) : \", len(eval(f\"{name}_all_dataset\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbf6ba",
   "metadata": {},
   "source": [
    "### 5) 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2223d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyerpt_rpt_preprocessing(\"train_eyerpt\")\n",
    "eyerpt_rpt_preprocessing(\"test_eyerpt\")\n",
    "eyerpt_rpt_preprocessing(\"train_rpt\")\n",
    "eyerpt_rpt_preprocessing(\"test_rpt\")\n",
    "\n",
    "print(\"len(train_eyerpt_all_dataset) : \", len(train_eyerpt_all_dataset))\n",
    "print(\"len(test_eyerpt_all_dataset) : \", len(test_eyerpt_all_dataset))\n",
    "print(\"len(train_rpt_all_dataset) : \", len(train_rpt_all_dataset))\n",
    "print(\"len(test_rpt_all_dataset) : \", len(test_rpt_all_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13144fec",
   "metadata": {},
   "source": [
    "### 3) 데이터 자료형 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376aa6de",
   "metadata": {},
   "source": [
    "rpt_dataset['panel_num'] = rpt_dataset['panel_num'].astype(float)\n",
    "rpt_dataset['error'] = rpt_dataset['error'].astype(float)\n",
    "\n",
    "print(\"ㅡㅡㅡㅡㅡ[eyerpt_dataset.dtypes]ㅡㅡㅡㅡㅡ\")\n",
    "print(eyerpt_dataset.dtypes)\n",
    "print(\"ㅡㅡㅡㅡㅡ[rpt_dataset.dtypes]ㅡㅡㅡㅡㅡ\")\n",
    "print(rpt_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8596e3",
   "metadata": {},
   "source": [
    "## 5. Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38c8c9",
   "metadata": {},
   "source": [
    "### 1) Load model epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "start_epoch = 0\n",
    "if args.resume:\n",
    "    start_epoch = load_checkpoint(args.checkpoints, args.title, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e6a61",
   "metadata": {},
   "source": [
    "### 2) Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, args.epochs):\n",
    "    \"\"\"\n",
    "    # start timer\n",
    "    start_time = time.time() # 확인용 코드\n",
    "    \"\"\"\n",
    "    # 모델 학습 소요시간\n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    # train one epoch + evaluate one epoch\n",
    "    train_summary = train(train_rpt_all_dataset.items(), args.learn_type, args.input_size, model, optimizer, scheduler, loss_fn, metric_fn, args.device)\n",
    "\n",
    "    # write log\n",
    "    train_logger.add_scalar('Loss', train_summary['loss'], epoch + 1)\n",
    "    if(args.learn_type == \"regression\"):\n",
    "        pass\n",
    "    else:\n",
    "        train_logger.add_scalar('Accuracy', train_summary['metric'], epoch + 1)\n",
    "    \n",
    "    # save model\n",
    "    save_checkpoint(args.checkpoints, args.title, model, optimizer, epoch + 1)\n",
    "        \n",
    "    # 모델 학습 소요시간\n",
    "    end_time = time.monotonic()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Print log\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    if(args.learn_type == \"regression\"):\n",
    "        print(f'\\t Train Loss: {train_summary[\"loss\"]:.3f}')\n",
    "    else:\n",
    "        print(f'\\t Train Loss: {train_summary[\"loss\"]:.3f} | Train Acc: {train_summary[\"metric\"]:.2f}%')\n",
    "    print(f'\\t scheduled_lr : {scheduler.get_last_lr()[0]}')\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), f\"{args.title}.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744bc792",
   "metadata": {},
   "source": [
    "## 6. Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 불러오기\n",
    "model.load_state_dict(torch.load(f\"{args.title}.ckpt\"))\n",
    "\n",
    "# 모델 성능 측정\n",
    "test_summary = evaluate(test_rpt_all_dataset.items(), args.learn_type, args.input_size, model, loss_fn, metric_fn, args.device)\n",
    "\n",
    "# write log\n",
    "test_logger.add_scalar('Loss', test_summary['loss'], epoch + 1)\n",
    "if(args.learn_type == \"regression\"):\n",
    "    pass\n",
    "else:\n",
    "    test_logger.add_scalar('Accuracy', test_summary['metric'], epoch + 1)\n",
    "\n",
    "if(args.learn_type == \"regression\"):\n",
    "    print(f'\\t Test Loss: {test_summary[\"loss\"]:.3f}')\n",
    "else:\n",
    "    print(f'\\t Test Loss: {test_summary[\"loss\"]:.3f} | Test Acc: {test_summary[\"metric\"]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224cc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BangEnv",
   "language": "python",
   "name": "bangenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
